1. create table POSTGRES

CREATE TABLE users (
    user_id SERIAL PRIMARY KEY,
    username VARCHAR(50) NOT NULL,
    account_type VARCHAR(20) NOT NULL,
    updated_at TIMESTAMP DEFAULT timezone('UTC', CURRENT_TIMESTAMP),
    created_at TIMESTAMP DEFAULT timezone('UTC', CURRENT_TIMESTAMP)
);

INSERT INTO users (username, account_type) VALUES
('user1', 'Bronze'),
('user2', 'Silver'),
('user3', 'Gold');


2. create debiuzim connector to kafka

curl --location --request POST 'http://localhost:8083/connectors' --header 'Content-Type: application/json' --data-raw '{
    "name": "shop-connector",
    "config": {
      "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
      "database.dbname": "postgres",
      "database.history.kafka.bootstrap.servers": "kafka:9092",
      "database.history.kafka.topic": "schema-changes.shop",
      "database.history.skip.unparseable.ddl": "true",
      "database.hostname": "postgres",
      "database.password": "postgres",
      "database.port": "5432",
      "database.server.name": "shop",
      "database.user": "postgres",
      "key.converter": "io.confluent.connect.avro.AvroConverter",
      "key.converter.schema.registry.url": "http://schema-registry:8081",
      "name": "shop-connector",
      "plugin.name": "pgoutput",
      "snapshot.mode": "initial",
      "table.include.list": "public.users",
      "tasks.max": "1",
      "topic.creation.default.cleanup.policy": "delete",
      "topic.creation.default.partitions": "1",
      "topic.creation.default.replication.factor": "1",
      "topic.creation.default.retention.ms": "604800000",
      "topic.creation.enable": "true",
      "topic.prefix": "shop",
      "transforms": "unwrap",
      "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
      "value.converter": "io.confluent.connect.avro.AvroConverter",
      "value.converter.schema.registry.url": "http://schema-registry:8081"
}
}'

3. Clickhouse create table

CREATE DATABASE shop;

CREATE TABLE shop.users
(
    user_id UInt32,
    username String,
    account_type String,
    updated_at DateTime,
    created_at DateTime,
    kafka_time Nullable(DateTime),
    kafka_offset UInt64
)
ENGINE = ReplacingMergeTree
ORDER BY (user_id, updated_at)
SETTINGS index_granularity = 8192;

4. Create kafka Engine table

CREATE DATABASE kafka_shop;

CREATE TABLE kafka_shop.kafka__users
(
    user_id UInt32,
    username String,
    account_type String,
    updated_at UInt64,
    created_at UInt64
)
ENGINE = Kafka
SETTINGS kafka_broker_list = 'broker:29092',
kafka_topic_list = 'shop.public.users',
kafka_group_name = 'clickhouse',
kafka_format = 'AvroConfluent',
format_avro_schema_registry_url='http://schema-registry:8081';


5. Create view for kafka connector

CREATE MATERIALIZED VIEW kafka_shop.consumer__users TO shop.users
(
    user_id UInt32,
    username String,
    account_type String,
    updated_at DateTime,
    created_at DateTime,
    kafka_time Nullable(DateTime),
    kafka_offset UInt64
) AS
SELECT
    user_id,
    username,
    account_type,
    toDateTime(updated_at / 1000000) AS updated_at,
    toDateTime(created_at / 1000000) AS created_at,
    _timestamp AS kafka_time,
    _offset AS kafka_offset
FROM kafka_shop.kafka__users;

